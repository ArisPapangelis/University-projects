setwd("C:/Users/FrenzyCawk/Desktop/Lab 1. Introduction to R")
cor(iris[1:4])
rm(list=ls())
# Read dataset into data frame
dn = list(paste("Y", as.character(1949:1960), sep = ""), month.abb)
airmat = matrix(AirPassengers, 12, byrow = TRUE, dimnames = dn)
air = as.data.frame(t(airmat))
air
cor(air)
rm(list=ls())
# Read dataset into data frame
dn = list(paste("Y", as.character(1949:1960), sep = ""), month.abb)
airmat = matrix(AirPassengers, 12, byrow = TRUE, dimnames = dn)
air = as.data.frame(t(airmat))
# How many passengers have traveled on average on 1951?
mean(air$Y1951)
# What is the maximum passenger number for January and February?
max(c(unlist(air["Jan",]), unlist(air["Feb",])))
# Compute the cross correlation for all data frame dimensions
cor(air)
# Compute sum for every year and plot it
x = colSums(air)
plot(x, type = 'b', xaxt = 'n', xlab = "", ylab = "#Passengers")
axis(1, at = 1:12, labels = names(x), las = 2)
# Compute sum for every month and all years and plot it
x = rowSums(air)
library(neuralnet)
detach("package:neuralnet", unload = TRUE)
setwd("C:/Users/FrenzyCawk/Desktop/Lab 2. Decision Trees")
rm(list=ls())
# Import decision tree libraries
library(rpart)
library(rpart.plot)
# Load iris dataset
iris2 = iris[, c(1, 2, 5)]
iris2$Species[c(101:150)] = iris2$Species[c(21:70)]
iris2$Species = factor(iris2$Species)
View(iris2)
rm(list=ls())
# Import decision tree libraries
library(rpart)
library(rpart.plot)
# Load iris dataset
iris2 = iris[, c(1, 2, 5)]
iris2$Species[c(101:150)] = iris2$Species[c(21:70)]
View(iris2)
iris2$Species = factor(iris2$Species)
View(iris2)
?factor
setwd("C:/Users/FrenzyCawk/Desktop/Lab 3. Naive Bayes")
setwd("C:/Users/FrenzyCawk/Desktop/Lab 5. kNN, SVM")
rm(list=ls())
alldata = read.csv("alldata.txt")
trainingdata = alldata[1:600,]
testdata = alldata[601:800,]
library(MLmetrics)
library(e1071)
#Question a
plot(trainingdata[,c(1:2)], col = trainingdata$y, pch = c("o","+")[trainingdata$y])
#Question b and c
X1 = seq(min(trainingdata[,1]), max(trainingdata[,1]), by = 0.1)
X2 = seq(min(trainingdata[,2]), max(trainingdata[,2]), by = 0.1)
mygrid = expand.grid(X1,X2)
colnames(mygrid) = colnames(trainingdata)[1:2]
#Gamma 1
svm_model <- svm(y ~ ., kernel = "radial", type = "C-classification", data = trainingdata, gamma = 1)
pred = predict(svm_model, mygrid)
Y = matrix(pred, length(X1), length(X2))
contour(X1,X2,Y, add = TRUE, levels = 1.5, labels = "gamma = 1", col = "blue")
#Gamma 0.01
svm_model <- svm(y ~ ., kernel = "radial", type = "C-classification", data = trainingdata, gamma = 0.01)
pred = predict(svm_model, mygrid)
Y = matrix(pred, length(X1), length(X2))
contour(X1,X2,Y, add = TRUE, levels = 1.5, labels = "gamma = 0.01", col = "red")
#Gamma 100
svm_model <- svm(y ~ ., kernel = "radial", type = "C-classification", data = trainingdata, gamma = 100)
pred = predict(svm_model, mygrid)
Y = matrix(pred, length(X1), length(X2))
contour(X1,X2,Y, add = TRUE, levels = 1.5, labels = "gamma = 100", col = "green")
#Question d
gammavalues = c(0.01, 0.1, 1, 10, 100, 1000, 10000, 100000, 1000000)
training_error = c()
for (gamma in gammavalues){
svm_model = svm(y ~ ., kernel="radial", type = "C-classification", data = trainingdata, gamma = gamma)
pred = predict(svm_model, trainingdata[,c(1:2)])
training_error = c(training_error, 1-Accuracy(pred,trainingdata$y))
}
testing_error = c()
for (gamma in gammavalues){
svm_model = svm(y ~ ., kernel="radial", type = "C-classification", data = trainingdata, gamma = gamma)
pred = predict(svm_model, testdata[,c(1:2)])
testing_error = c(testing_error, 1-Accuracy(pred,testdata$y))
}
#Question e
plot(training_error, type = "l", col="blue", ylim = c(0, 0.5), xlab = "Gamma", ylab = "Error", xaxt = "n")
axis(1, at = 1:length(gammavalues), labels = gammavalues)
lines(testing_error, col="red")
legend("right", c("Training Error", "Testing Error"), pch = c("-","-"), col = c("blue", "red"))
#Question f
k = 10
dsize = nrow(trainingdata)
folds = split(sample(1:dsize), ceiling(seq(dsize) * k / dsize))
accuracies = c()
for (gamma in gammavalues){
predictions = data.frame()
testsets = data.frame()
for (i in 1:k){
trainingset = trainingdata[unlist(folds[-i]),]
validationset = trainingdata[unlist(folds[i]),]
svm_model = svm(y ~ ., kernel="radial", type = "C-classification", data = trainingset, gamma = gamma)
pred = predict(svm_model, validationset[, c(1:2)])
predictions = rbind(predictions, as.data.frame(pred))
testsets = rbind(testsets, as.data.frame(validationset[,3]))
}
accuracies = c(accuracies, Accuracy(predictions,testsets))
}
print(accuracies)
bestgamma = gammavalues[which.max(accuracies)]
rm(list=ls())
X1 = c(2, 2, -2, -2, 1, 1, -1, -1)
X2 = c(2, -2, -2, 2, 1, -1, -1, 1)
Y = c(1, 1, 1, 1, 2, 2, 2, 2)
alldata = data.frame(X1, X2, Y)
plot(alldata[,c("X1","X2")], col = alldata$Y, pch = c("o","+")[alldata$y])
library(e1071)
library(MLmetrics)
#Question 1
model <- svm(Y ~ ., kernel = "radial",  type = "C-classification", data = alldata, gamma = 1)
pred = predict(model,alldata[,c(1:2)])
Accuracy(pred,alldata$Y)
#Question 2
#Buggy for some reason with probability
#model <- svm(Y ~ ., kernel = "radial",  type = "C-classification", data = alldata, gamma = 1000000, probability = TRUE)
#new_data = data.frame(X1=-2, X2 = -1.9)
#predict(model, new_data, probability = TRUE)
model <- svm(Y ~ ., kernel = "radial",  type = "C-classification", data = alldata, gamma = 1000000)
new_data = data.frame(X1=-2, X2 = -1.9)
predict(model, new_data)
#Question 3
# plot classified point
points(-2, -1.9, col = "green", pch = 9)
# create vector for different gamma values
gammavalues = c(0.01, 0.1, 1, 10, 100, 1000, 10000, 100000, 1000000)
# create sequences for data
X1 = seq(min(alldata[, 1]), max(alldata[, 1]), by = 0.1)
X2 = seq(min(alldata[, 2]), max(alldata[, 2]), by = 0.1)
# create data grid
mygrid = expand.grid(X1, X2)
colnames(mygrid) = colnames(alldata)[1:2]
for (gamma in gammavalues) {
# create svm model
svm_model = svm(Y ~ ., kernel="radial", type="C-classification", data = alldata, gamma = gamma)
# run model prediction
pred = predict(svm_model, mygrid)
# create matrix to give as z argument to the contour
Y = matrix(pred, length(X1), length(X2))
# plot contour with SVM classifier on top of data
contour(X1, X2, Y, add = TRUE, labels = paste("gamma = ", toString(gamma)), levels = 1.5, col = gamma)
}
rm(list=ls())
X1 = c(2, 2, -2, -2, 1, 1, -1, -1)
X2 = c(2, -2, -2, 2, 1, -1, -1, 1)
Y = c(1, 1, 1, 1, 2, 2, 2, 2)
alldata = data.frame(X1, X2, Y)
plot(alldata[,c("X1","X2")], col = alldata$Y, pch = c("o","+")[alldata$y])
library(e1071)
library(MLmetrics)
#Question 1
model <- svm(Y ~ ., kernel = "radial",  type = "C-classification", data = alldata, gamma = 1)
pred = predict(model,alldata[,c(1:2)])
Accuracy(pred,alldata$Y)
#Question 2
#Buggy for some reason with probability
#model <- svm(Y ~ ., kernel = "radial",  type = "C-classification", data = alldata, gamma = 1000000, probability = TRUE)
#new_data = data.frame(X1=-2, X2 = -1.9)
#predict(model, new_data, probability = TRUE)
model <- svm(Y ~ ., kernel = "radial",  type = "C-classification", data = alldata, gamma = 1000000)
new_data = data.frame(X1=-2, X2 = -1.9)
predict(model, new_data)
#Question 3
# plot classified point
points(-2, -1.9, col = "green", pch = 9)
# create vector for different gamma values
gammavalues = c(0.01, 0.1, 1, 10, 100, 1000, 10000, 100000, 1000000)
# create sequences for data
X1 = seq(min(alldata[, 1]), max(alldata[, 1]), by = 0.1)
X2 = seq(min(alldata[, 2]), max(alldata[, 2]), by = 0.1)
# create data grid
mygrid = expand.grid(X1, X2)
colnames(mygrid) = colnames(alldata)[1:2]
for (gamma in gammavalues) {
# create svm model
svm_model = svm(Y ~ ., kernel="radial", type="C-classification", data = alldata, gamma = gamma)
# run model prediction
pred = predict(svm_model, mygrid)
# create matrix to give as z argument to the contour
Y = matrix(pred, length(X1), length(X2))
# plot contour with SVM classifier on top of data
contour(X1, X2, Y, add = TRUE, labels = paste("gamma = ", toString(gamma)), levels = 1.5, col = gamma)
}
